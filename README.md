# Azure Data Engineering Project

## ğŸ“Œ Project Overview
This project demonstrates an **end-to-end data engineering solution** built using Microsoft Azure services. It covers data ingestion, storage, transformation, loading, and reporting for a simulated on-premise SQL Server dataset.  
Developed as part of a Hands-On to strengthen practical data engineering skills.

---

## ğŸ—ï¸ Architecture Diagram

![Architecture Diagram](images/architecture.png)

---

## ğŸ’» Tools & Technologies Used

- Azure Data Factory (ADF)
- Azure Data Lake Storage Gen2
- Azure Databricks (PySpark)
- Azure Synapse Analytics
- Microsoft Power BI
- Azure Key Vault
- Microsoft Entra ID

---

## ğŸ”¨ Implementation Steps

### 1. Data Ingestion
- Created linked services in **ADF** to connect to on-premise SQL Server.
- Built data pipelines to ingest multiple tables into **Azure Data Lake Storage Gen2**.

### 2. Data Storage
- Organized raw data in a **RAW container** in ADLS Gen2 with structured folders by table name.

### 3. Data Transformation
- Used **Azure Databricks** with PySpark notebooks to clean and transform raw data into curated datasets.
- Saved cleaned data in a **CURATED container** in ADLS Gen2.

### 4. Data Loading
- Loaded transformed data into **Azure Synapse Analytics** tables for optimized querying.

### 5. Reporting
- Built interactive dashboards in **Power BI** connected to Synapse Analytics for insights.

### 6. Governance & Security
- Managed secrets and connection strings securely using **Azure Key Vault**.
- Configured access control with **Microsoft Entra ID** for monitoring and governance.

### 7. End-to-End Testing
- Validated pipelines to ensure seamless data flow from ingestion to reporting.

---

## ğŸ“ Repository Structure

```
azure-data-engineering-project/
â”‚
â”œâ”€â”€ images/
â”‚   â””â”€â”€ architecture.png
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ databricks_transformation.ipynb
â”‚
â”œâ”€â”€ pipeline_json/
â”‚   â””â”€â”€ adf_pipeline_export.json
â”‚
â””â”€â”€ README.md
```



---

## ğŸ“‚ Sample Files Included

- `notebooks/databricks_transformation.ipynb` â€“ PySpark data transformation notebook.
- `pipeline_json/adf_pipeline_export.json` â€“ Exported ADF pipeline JSON definition.
- `images/architecture.png` â€“ Project architecture diagram.


---

## ğŸ¯ Learning Outcomes

âœ”ï¸ Built scalable pipelines using Azure Data Factory  
âœ”ï¸ Transformed data efficiently with PySpark in Databricks  
âœ”ï¸ Designed data warehousing solutions in Synapse Analytics  
âœ”ï¸ Created impactful Power BI dashboards  
âœ”ï¸ Applied governance with Key Vault and Entra ID  
âœ”ï¸ Strengthened practical cloud data engineering skills

---

## ğŸ“Œ Author

**Sudhanshu Pukale**

## ğŸš€ Next Steps

- Automate deployment using ARM templates or Terraform  
- Integrate CI/CD for pipeline deployment  
- Enhance security with Managed Identities and Private Endpoints


